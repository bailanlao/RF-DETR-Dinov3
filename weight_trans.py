import torch
from rfdetr.models.backbone.dinov2 import DinoV2
from rfdetr.models.backbone.dinov3 import DinoV3
from rfdetr import RFDETRNano, RFDETRBase, RFDETRMedium, RFDETRLarge, RFDETRMediumV3,RFDETRNanoV3,RFDETRMediumV3Plus
from torch.nn import Module
from typing import Dict, Tuple

import torch
from torch.nn import Module
from typing import Tuple, List, Dict


def count_module_params(module: Module) -> int:
    return sum(param.numel() for param in module.parameters())


def transfer_rfdetr_to_dinov3_weights(
    rfdetr_core_model: Module,
    dinov3_core_model: Module,
    device: torch.device = None,
    transfer_top_heads: bool = True
) -> Tuple[Module, List[str], int]:
    """
    æ ¸å¿ƒå‡½æ•°ï¼šå°†rfdetr_core_modelçš„å¯è¿ç§»æ¨¡å—æƒé‡è¿ç§»åˆ°dinov3_core_model
    å¯è¿ç§»æ¨¡å—ï¼šDecoderã€TransformerEncoderç›¸å…³æ¨¡å—ã€Backbone Projectorã€ï¼ˆå¯é€‰ï¼‰é¡¶å±‚ä»»åŠ¡å¤´
    
    Args:
        rfdetr_core_model: æƒé‡æ¥æºæ¨¡å‹ï¼ˆrfdetrçš„æ ¸å¿ƒæ¨¡å‹ï¼Œå³rfdetr.model.modelï¼‰
        dinov3_core_model: æƒé‡ç›®æ ‡æ¨¡å‹ï¼ˆdinov3çš„æ ¸å¿ƒæ¨¡å‹ï¼Œå³model.model.modelï¼‰
        device: æ¨¡å‹è¿è¡Œè®¾å¤‡ï¼ˆé»˜è®¤è‡ªåŠ¨é€‰æ‹©GPU/CPUï¼‰
        transfer_top_heads: æ˜¯å¦è¿ç§»é¡¶å±‚ä»»åŠ¡å¤´ï¼ˆclass_embed/bbox_embed/refpoint_embed/query_featï¼‰ï¼Œé»˜è®¤True
    
    Returns:
        Tuple[Module, List[str], int]:
            - è¿ç§»åçš„dinov3_core_modelï¼ˆå·²åŠ è½½æƒé‡ï¼‰
            - è¿ç§»æ—¥å¿—åˆ—è¡¨ï¼ˆè®°å½•æ¯ä¸ªæ¨¡å—çš„è¿ç§»ç»“æœå’Œå‚æ•°é‡ï¼‰
            - æ€»è¿ç§»å‚æ•°é‡ï¼ˆå…ƒç´ æ€»æ•°ï¼‰
    
    Raises:
        AssertionError: æ¨¡å‹ç»“æ„ä¸åŒ¹é…æ—¶æŠ›å‡ºï¼ˆå¦‚å…³é”®æ¨¡å—ç¼ºå¤±ï¼‰
        RuntimeError: æƒé‡åŠ è½½å¤±è´¥æ—¶æŠ›å‡ºï¼ˆå¦‚å‚æ•°å½¢çŠ¶ä¸å…¼å®¹ï¼‰
    """
    # 1. åˆå§‹åŒ–è®¾å¤‡ï¼ˆé»˜è®¤è‡ªåŠ¨é€‰æ‹©ï¼‰
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[æƒé‡è¿ç§»] ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # 2. ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹éƒ½åœ¨æŒ‡å®šè®¾å¤‡ä¸Š
    rfdetr_core = rfdetr_core_model.to(device).eval()  # evalæ¨¡å¼é¿å…BatchNormç­‰å±‚å¹²æ‰°
    dinov3_core = dinov3_core_model.to(device).eval()
    total_transferred = 0
    transfer_log: List[str] = []

    # -------------------------- 3. è¿ç§» Transformer Decoderï¼ˆå¿…è¿æ¨¡å—ï¼‰ --------------------------
    try:
        src_decoder = rfdetr_core.transformer.decoder
        dst_decoder = dinov3_core.transformer.decoder
        dst_decoder.load_state_dict(src_decoder.state_dict(), strict=True)
        params = count_module_params(src_decoder)
        total_transferred += params
        log = f"âœ… [Transformer Decoder] è¿ç§»å®Œæˆ | å‚æ•°é‡ï¼š{params:,}"
        transfer_log.append(log)
        print(log)
    except Exception as e:
        raise RuntimeError(f"[Transformer Decoder] è¿ç§»å¤±è´¥ï¼š{str(e)}") from e

    # -------------------------- 4. è¿ç§» Transformer Encoder ç›¸å…³æ¨¡å—ï¼ˆå¿…è¿æ¨¡å—ï¼‰ --------------------------
    encoder_related_modules: Dict[str, str] = {
        "enc_output": "transformer.enc_output",
        "enc_output_norm": "transformer.enc_output_norm",
        "enc_out_bbox_embed": "transformer.enc_out_bbox_embed",
        "enc_out_class_embed": "transformer.enc_out_class_embed"
    }
    for module_name, module_path in encoder_related_modules.items():
        try:
            # é€šè¿‡å­—ç¬¦ä¸²è·¯å¾„è·å–æ¨¡å—ï¼ˆé¿å…ç¡¬ç¼–ç å±‚çº§ï¼‰
            src_module = eval(f"rfdetr_core.{module_path}")
            dst_module = eval(f"dinov3_core.{module_path}")
            dst_module.load_state_dict(src_module.state_dict(), strict=True)
            params = count_module_params(src_module)
            total_transferred += params
            log = f"âœ… [{module_name}] è¿ç§»å®Œæˆ | å‚æ•°é‡ï¼š{params:,}"
            transfer_log.append(log)
            print(log)
        except Exception as e:
            raise RuntimeError(f"[{module_name}] è¿ç§»å¤±è´¥ï¼š{str(e)}") from e

    # -------------------------- 5. è¿ç§» Backbone Projectorï¼ˆå¿…è¿æ¨¡å—ï¼‰ --------------------------
    try:
        # Backboneæ˜¯Joinerç±»å‹ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å®é™…Backboneå®ä¾‹ï¼ˆå«projectorï¼‰
        src_projector = rfdetr_core.backbone[0].projector
        dst_projector = dinov3_core.backbone[0].projector
        dst_projector.load_state_dict(src_projector.state_dict(), strict=True)
        params = count_module_params(src_projector)
        total_transferred += params
        log = f"âœ… [Backbone Projector] è¿ç§»å®Œæˆ | å‚æ•°é‡ï¼š{params:,}"
        transfer_log.append(log)
        print(log)
    except Exception as e:
        raise RuntimeError(f"[Backbone Projector] è¿ç§»å¤±è´¥ï¼š{str(e)}") from e

    # -------------------------- 6. å¯é€‰è¿ç§»ï¼šé¡¶å±‚ä»»åŠ¡å¤´ --------------------------
    if transfer_top_heads:
        top_head_modules: Dict[str, str] = {
            "class_embed": "class_embed",
            "bbox_embed": "bbox_embed",
            "refpoint_embed": "refpoint_embed",
            "query_feat": "query_feat"
        }
        for module_name, module_path in top_head_modules.items():
            try:
                src_module = eval(f"rfdetr_core.{module_path}")
                dst_module = eval(f"dinov3_core.{module_path}")
                dst_module.load_state_dict(src_module.state_dict(), strict=True)
                params = count_module_params(src_module)
                total_transferred += params
                log = f"âœ… [é¡¶å±‚ä»»åŠ¡å¤´-{module_name}] è¿ç§»å®Œæˆ | å‚æ•°é‡ï¼š{params:,}"
                transfer_log.append(log)
                print(log)
            except Exception as e:
                raise RuntimeError(f"[é¡¶å±‚ä»»åŠ¡å¤´-{module_name}] è¿ç§»å¤±è´¥ï¼š{str(e)}") from e
    else:
        log = "â„¹ï¸ [å¯é€‰é…ç½®] æœªè¿ç§»é¡¶å±‚ä»»åŠ¡å¤´ï¼ˆtransfer_top_heads=Falseï¼‰"
        transfer_log.append(log)
        print(log)

    # -------------------------- 7. è¾“å‡ºè¿ç§»æ€»ç»“ --------------------------
    summary_log = f"\n[è¿ç§»æ€»ç»“] æ€»è¿ç§»å‚æ•°é‡ï¼š{total_transferred:,} ä¸ªï¼ˆ{total_transferred / 1e6:.2f} Mï¼‰"
    transfer_log.append(summary_log)
    print("="*80)
    print(summary_log)
    print("âš ï¸  ä¸å¯è¿ç§»æ¨¡å—ï¼šbackbone.encoderï¼ˆDinoV2 vs DinoV3 ç»“æ„ä¸å…¼å®¹ï¼‰")
    print("="*80)

    return dinov3_core, transfer_log, total_transferred

def transfer_dinov3_to_core_model(
    dinov3_core_model: Module,
    pretrained_dinov3: Module,
    device: torch.device = None
) -> Tuple[Module, List[str], int]:
    """
    å°†é¢„è®­ç»ƒdinov3æ¨¡å‹çš„æƒé‡è¿ç§»åˆ°dinov3_core_modelçš„ç¼–ç å™¨ä¸­
    è‡ªåŠ¨å®šä½ç¼–ç å™¨è·¯å¾„ï¼šdinov3_core_model.backbone[0].encoder.encoder.encoder
    
    Args:
        dinov3_core_model: ç›®æ ‡æ ¸å¿ƒæ¨¡å‹ï¼ˆå«å¾…è¿ç§»çš„ç¼–ç å™¨ï¼‰
        pretrained_dinov3: é¢„è®­ç»ƒçš„dinov3æ¨¡å‹ï¼ˆDinoVisionTransformerï¼‰
        device: è®¾å¤‡ï¼ˆé»˜è®¤è‡ªåŠ¨é€‰æ‹©ï¼‰
    
    Returns:
        è¿ç§»åçš„dinov3_core_modelã€è¿ç§»æ—¥å¿—ã€æ€»è¿ç§»å‚æ•°é‡
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 1. å°†æ¨¡å‹ç§»è‡³æŒ‡å®šè®¾å¤‡
    dinov3_core_model = dinov3_core_model.to(device).eval()
    pretrained_dinov3 = pretrained_dinov3.to(device).eval()
    
    # 2. å®šä½ç›®æ ‡ç¼–ç å™¨ï¼ˆdinov3_core_modelä¸­çš„ç¼–ç å™¨ï¼‰
    try:
        dinov3_encoder = dinov3_core_model.backbone[0].encoder.encoder.encoder
        print(f"âœ… æˆåŠŸå®šä½ç¼–ç å™¨ï¼š{type(dinov3_encoder).__name__}")
    except AttributeError as e:
        raise RuntimeError(f"æ— æ³•å®šä½ç¼–ç å™¨è·¯å¾„ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„ï¼š{str(e)}") from e
    
    # 3. æå–é¢„è®­ç»ƒæ¨¡å‹çš„æ ¸å¿ƒå±‚ï¼ˆ12ä¸ªSelfAttentionBlockï¼‰
    src_blocks = pretrained_dinov3.blocks
    dst_layers = dinov3_encoder.layer  # ç›®æ ‡ç¼–ç å™¨çš„12ä¸ªå±‚
    
    # éªŒè¯å±‚æ•°é‡æ˜¯å¦åŒ¹é…ï¼ˆå‡ä¸º12å±‚ï¼‰
    assert len(src_blocks) == len(dst_layers), \
        f"æ ¸å¿ƒå±‚æ•°é‡ä¸åŒ¹é…ï¼šé¢„è®­ç»ƒæ¨¡å‹æœ‰{len(src_blocks)}å±‚ï¼Œç›®æ ‡ç¼–ç å™¨æœ‰{len(dst_layers)}å±‚"
    
    total_params = 0
    transfer_log = []
    
    # 4. é€å±‚è¿ç§»å‚æ•°ï¼ˆ12å±‚ä¸€ä¸€å¯¹åº”ï¼‰
    for i in range(len(src_blocks)):
        src_block = src_blocks[i]
        dst_layer = dst_layers[i]
        layer_log = []
        layer_total = 0
        
        # è¿ç§»å„ç»„ä»¶ï¼ˆç»“æ„å®Œå…¨åŒ¹é…ï¼‰
        components = [
            ("norm1", src_block.norm1, dst_layer.norm1),
            ("attn", src_block.attn, dst_layer.attn),
            ("ls1", src_block.ls1, dst_layer.ls1),
            ("norm2", src_block.norm2, dst_layer.norm2),
            ("mlp", src_block.mlp, dst_layer.mlp),
            ("ls2", src_block.ls2, dst_layer.ls2)
        ]
        
        for comp_name, src_comp, dst_comp in components:
            try:
                dst_comp.load_state_dict(src_comp.state_dict(), strict=True)
                params = count_module_params(src_comp)
                layer_total += params
                layer_log.append(f"{comp_name}ï¼ˆ{params:,}ï¼‰")
            except Exception as e:
                raise RuntimeError(f"ç¬¬{i}å±‚{comp_name}è¿ç§»å¤±è´¥ï¼š{str(e)}")
        
        # è®°å½•å½“å‰å±‚è¿ç§»ç»“æœ
        total_params += layer_total
        log = f"âœ… ç¬¬{i}å±‚è¿ç§»å®Œæˆ | ç»„ä»¶ï¼š{', '.join(layer_log)} | æ€»å‚æ•°é‡ï¼š{layer_total:,}"
        transfer_log.append(log)
        print(log)
    
    # 5. è¿ç§»æ€»ç»“ï¼ˆç¼–ç å™¨å·²åµŒå…¥å›dinov3_core_modelï¼‰
    summary = f"\nè¿ç§»å®Œæˆ | æ€»å‚æ•°é‡ï¼š{total_params:,}ï¼ˆ{total_params/1e6:.2f} Mï¼‰"
    transfer_log.append(summary)
    print("="*80)
    print(summary)
    print(f"ç¼–ç å™¨å·²æ›´æ–°è‡³ dinov3_core_model.backbone[0].encoder.encoder.encoder")
    print("="*80)
    
    return dinov3_core_model, transfer_log, total_params

import torch
from collections import OrderedDict

def transfer_rf_to_dinov3(rf_model, dinov3_model):
    """
    å°†RFï¼ˆDinoV2éª¨å¹²ï¼‰æ¨¡å‹çš„å‚æ•°è¿ç§»åˆ°Dinov3æ¨¡å‹ä¸­ï¼Œä»…è¿ç§»embeddingså’Œprojectoræ¨¡å—ï¼Œä¸åŒ…å«encoder.layerç›¸å…³å‚æ•°ã€‚
    
    Args:
        rf_model: RF-DETRçš„Backboneæ¨¡å—ï¼ˆrfdetr_core_model.backbone[0]ï¼‰
        dinov3_model: Dinov3çš„Backboneæ¨¡å—ï¼ˆdinov3_core_model.backbone[0]ï¼‰
    
    Returns:
        torch.nn.Module: å‚æ•°è¿ç§»åçš„Dinov3æ¨¡å‹
    """
    rf_state_dict = rf_model.state_dict()
    dinov3_state_dict = dinov3_model.state_dict()
    new_dinov3_state = OrderedDict()

    # å‚æ•°æ˜ å°„ï¼šä»…åŒ…å«embeddingså’Œprojectoræ¨¡å—ï¼Œç§»é™¤æ‰€æœ‰encoder.layerç›¸å…³æ˜ å°„
    param_mapping = {
        # -------------------------- embeddingsæ¨¡å—ï¼ˆæ ¸å¿ƒè¿ç§»ï¼‰--------------------------
        # ä½ç½®ç¼–ç 
        "encoder.encoder.embeddings.position_embeddings": 
            "encoder.encoder.embeddings.position_embeddings",
        # CLSä»¤ç‰Œ
        "encoder.encoder.embeddings.cls_token": 
            "encoder.encoder.embeddings.cls_token",
        # æ©ç ä»¤ç‰Œ
        "encoder.encoder.embeddings.mask_token": 
            "encoder.encoder.embeddings.mask_token",
        # å¯„å­˜å™¨ä»¤ç‰Œ
        "encoder.encoder.embeddings.register_tokens": 
            "encoder.encoder.embeddings.register_tokens",
        # è¡¥ä¸åµŒå…¥
        "encoder.encoder.embeddings.patch_embeddings.projection.weight": 
            "encoder.encoder.embeddings.patch_embeddings.projection.weight",
        "encoder.encoder.embeddings.patch_embeddings.projection.bias": 
            "encoder.encoder.embeddings.patch_embeddings.projection.bias",
        
        # -------------------------- æœ€ç»ˆå±‚å½’ä¸€åŒ–ï¼ˆélayeréƒ¨åˆ†ï¼‰--------------------------
        "encoder.encoder.layernorm.weight": 
            "encoder.encoder.layernorm.weight",
        "encoder.encoder.layernorm.bias": 
            "encoder.encoder.layernorm.bias",
        
        # -------------------------- Projectoræ¨¡å— --------------------------
        "projector.stages.0.0.cv1.conv.weight": 
            "projector.stages.0.0.cv1.conv.weight",
        "projector.stages.0.0.cv1.bn.weight": 
            "projector.stages.0.0.cv1.bn.weight",
        "projector.stages.0.0.cv1.bn.bias": 
            "projector.stages.0.0.cv1.bn.bias",
        "projector.stages.0.0.cv2.conv.weight": 
            "projector.stages.0.0.cv2.conv.weight",
        "projector.stages.0.0.cv2.bn.weight": 
            "projector.stages.0.0.cv2.bn.weight",
        "projector.stages.0.0.cv2.bn.bias": 
            "projector.stages.0.0.cv2.bn.bias",
        # Bottleneckæ¨¡å—ï¼ˆ3ä¸ªï¼‰
        "projector.stages.0.0.m.{m}.cv1.conv.weight": 
            "projector.stages.0.0.m.{m}.cv1.conv.weight",
        "projector.stages.0.0.m.{m}.cv1.bn.weight": 
            "projector.stages.0.0.m.{m}.cv1.bn.weight",
        "projector.stages.0.0.m.{m}.cv1.bn.bias": 
            "projector.stages.0.0.m.{m}.cv1.bn.bias",
        "projector.stages.0.0.m.{m}.cv2.conv.weight": 
            "projector.stages.0.0.m.{m}.cv2.conv.weight",
        "projector.stages.0.0.m.{m}.cv2.bn.weight": 
            "projector.stages.0.0.m.{m}.cv2.bn.weight",
        "projector.stages.0.0.m.{m}.cv2.bn.bias": 
            "projector.stages.0.0.m.{m}.cv2.bn.bias",
        # Projectoræœ€ç»ˆå±‚å½’ä¸€åŒ–
        "projector.stages.0.1.weight": 
            "projector.stages.0.1.weight",
        "projector.stages.0.1.bias": 
            "projector.stages.0.1.bias",
    }

    # 1. å¤„ç†Projectorä¸­çš„Bottleneckæ¨¡å—ï¼ˆå«{m}å¾ªç¯å˜é‡ï¼‰
    num_bottlenecks = 3  # å›ºå®š3ä¸ªBottleneck
    for m in range(num_bottlenecks):
        for dinov3_key_pattern, rf_key_pattern in param_mapping.items():
            if "{m}" not in dinov3_key_pattern:
                continue  # åªå¤„ç†å«{m}çš„å‚æ•°
            
            dinov3_key = dinov3_key_pattern.format(m=m)
            if dinov3_key not in dinov3_state_dict:
                continue  # Dinov3ä¸­æ— æ­¤å‚æ•°
            
            rf_key = rf_key_pattern.format(m=m)
            if rf_key not in rf_state_dict:
                continue  # RFä¸­æ— æ­¤å‚æ•°
            
            # éªŒè¯å½¢çŠ¶å¹¶è¿ç§»
            if rf_state_dict[rf_key].shape == dinov3_state_dict[dinov3_key].shape:
                new_dinov3_state[dinov3_key] = rf_state_dict[rf_key]
                print(f"âœ… è¿ç§»Bottleneck {m}ï¼š{dinov3_key}")
            else:
                print(f"âš ï¸ è·³è¿‡Bottleneck {m}ï¼š{dinov3_key}ï¼ˆå½¢çŠ¶ä¸åŒ¹é…ï¼‰")

    # 2. å¤„ç†æ— å¾ªç¯å˜é‡çš„æ¨¡å—ï¼ˆembeddingså’Œå›ºå®šç»“æ„ï¼‰
    for dinov3_key, rf_key in param_mapping.items():
        # è·³è¿‡å«å¾ªç¯å˜é‡çš„å‚æ•°ï¼ˆå·²å¤„ç†ï¼‰
        if "{m}" in dinov3_key:
            continue
        
        if dinov3_key not in dinov3_state_dict:
            continue  # Dinov3ä¸­æ— æ­¤å‚æ•°
        
        # ä»RFè·å–å‚æ•°ï¼ˆéå‡½æ•°ç±»å‹æ˜ å°„ï¼‰
        if not callable(rf_key) and rf_key in rf_state_dict:
            rf_param = rf_state_dict[rf_key]
            # éªŒè¯å½¢çŠ¶
            if rf_param.shape == dinov3_state_dict[dinov3_key].shape:
                new_dinov3_state[dinov3_key] = rf_param
                print(f"âœ… è¿ç§»ï¼š{dinov3_key}")
            else:
                print(f"âš ï¸ è·³è¿‡ï¼š{dinov3_key}ï¼ˆå½¢çŠ¶ä¸åŒ¹é…ï¼ŒRF: {rf_param.shape}, Dinov3: {dinov3_state_dict[dinov3_key].shape}ï¼‰")


    # 3. ä¿ç•™Dinov3æ‰€æœ‰æœªåŒ¹é…çš„å‚æ•°ï¼ˆå°¤å…¶æ˜¯encoder.layerç›¸å…³å‚æ•°ï¼‰
    for dinov3_key in dinov3_state_dict:
        if dinov3_key not in new_dinov3_state:
            new_dinov3_state[dinov3_key] = dinov3_state_dict[dinov3_key]

    # åŠ è½½è¿ç§»åçš„å‚æ•°
    dinov3_model.load_state_dict(new_dinov3_state, strict=False)
    print("\nğŸ‰ è¿ç§»å®Œæˆï¼å·²è·³è¿‡æ‰€æœ‰encoder.layerç›¸å…³å‚æ•°")
    return dinov3_model


if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    save_path='medium-dinov3plus.pth'
    model=RFDETRMediumV3Plus()
    rfdetr = RFDETRMedium(pretrain_weights='D:/__easyHelper__/RF-DETR/rfdetr/checkpoint/medium-coco.pth')
    dinov3=torch.hub.load(
        'D:/__easyHelper__/dinov3-main', 
        'dinov3_vits16plus', 
        source='local', 
        weights='D:/__easyHelper__/dinov3-main/checkpoint/dinov3_vits16plus.pth'
    )
    rfdetr_core_model=rfdetr.model.model.to(device)
    dinov3_core_model=model.model.model.to(device)
    dinov3_core_model, transfer_log, total_params = transfer_rfdetr_to_dinov3_weights(
        rfdetr_core_model=rfdetr_core_model,
        dinov3_core_model=dinov3_core_model,
        device=None,  # è‡ªåŠ¨é€‰æ‹©GPU/CPU
        transfer_top_heads=False  # è¿ç§»é¡¶å±‚ä»»åŠ¡å¤´
    ) # è¿ç§»éencoderçš„éƒ¨åˆ†
    transfer_rf_to_dinov3(
        rfdetr_core_model.backbone[0],
        dinov3_core_model.backbone[0],
    ) # è¿ç§»encoderä¸­å…¶ä»–éƒ¨åˆ†
    dinov3_encoder = dinov3_core_model.backbone[0].encoder.encoder.encoder

    print("dinov3å‚æ•°é‡ï¼š",count_module_params(dinov3))

    updated_core_model, transfer_log, total_params = transfer_dinov3_to_core_model(
        dinov3_core_model=dinov3_core_model,  # ç›®æ ‡æ ¸å¿ƒæ¨¡å‹
        pretrained_dinov3=dinov3,            # é¢„è®­ç»ƒdinov3æ¨¡å‹
        device=None  # è‡ªåŠ¨é€‰æ‹©GPU/CPU
    )
    torch.save({
            'model': dinov3_core_model.state_dict()
        }, save_path)
