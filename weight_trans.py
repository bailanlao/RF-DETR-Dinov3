import torch
from torch import nn
from rfdetr.models.backbone.dinov2 import DinoV2
from rfdetr.models.backbone.dinov3 import DinoV3
from rfdetr import RFDETRNano, RFDETRBase, RFDETRMedium, RFDETRLarge, RFDETRMediumV3, RFDETRNanoV3, RFDETRMediumV3Plus
from torch.nn import Module
from typing import Dict, Tuple, List, OrderedDict
import copy

def count_module_params(module: Module) -> int:
    return sum(param.numel() for param in module.parameters())


def get_nested_module(model: torch.nn.Module, path: str) -> torch.nn.Module:
    """å®‰å…¨è·å–åµŒå¥—æ¨¡å—ï¼Œè·¯å¾„æ ¼å¼å¦‚ "transformer.enc_output" """
    modules = path.split(".")
    current = model
    for mod_name in modules:
        current = getattr(current, mod_name, None)
        if current is None:
            raise AttributeError(f"æ¨¡å—è·¯å¾„ä¸å­˜åœ¨ï¼š{path}ï¼ˆé”™è¯¯å±‚çº§ï¼š{mod_name}ï¼‰")
    return current


def parameter_consistency_check(src_param: torch.Tensor, dst_param: torch.Tensor, param_name: str, threshold: float = 1e-6) -> Tuple[bool, float]:
    """
    æ£€æŸ¥ä¸¤ä¸ªå‚æ•°å¼ é‡çš„ä¸€è‡´æ€§
    è¿”å›ï¼š(æ˜¯å¦ä¸€è‡´, æœ€å¤§ç»å¯¹è¯¯å·®)
    """
    if src_param.shape != dst_param.shape:
        return False, -1.0  # å½¢çŠ¶ä¸åŒ¹é…
    
    # è®¡ç®—æœ€å¤§ç»å¯¹è¯¯å·®
    max_abs_error = torch.max(torch.abs(src_param - dst_param)).item()
    return max_abs_error < threshold, max_abs_error


def verify_encoder_layers(pretrained_dinov3: Module, target_model: Module, num_layers: int = 12) -> List[str]:
    """éªŒè¯ä»é¢„è®­ç»ƒDinoV3è¿ç§»åˆ°ç›®æ ‡æ¨¡å‹encoderçš„å‚æ•°æ˜¯å¦æˆåŠŸ"""
    log = []
    log.append("\n===== ç¼–ç å™¨å±‚å‚æ•°è¿ç§»éªŒè¯ =====")
    
    try:
        src_blocks = pretrained_dinov3.blocks
        dst_encoder = get_nested_module(target_model, "backbone.0.encoder.encoder.encoder")
        dst_layers = dst_encoder.layer
        
        if len(src_blocks) != len(dst_layers):
            log.append(f"âŒ ç¼–ç å™¨å±‚æ•°é‡ä¸åŒ¹é…ï¼šæº{len(src_blocks)}å±‚ï¼Œç›®æ ‡{len(dst_layers)}å±‚")
            return log
        
        # éªŒè¯æ¯ä¸€å±‚çš„å…³é”®ç»„ä»¶
        components = [
            ("norm1", lambda x: x.norm1),
            ("attn", lambda x: x.attn),
            ("ls1", lambda x: x.ls1),
            ("norm2", lambda x: x.norm2),
            ("mlp", lambda x: x.mlp),
            ("ls2", lambda x: x.ls2)
        ]
        
        all_passed = True
        
        for layer_idx in range(min(num_layers, len(src_blocks))):
            src_block = src_blocks[layer_idx]
            dst_layer = dst_layers[layer_idx]
            layer_passed = True
            
            log.append(f"\n--- ç¬¬{layer_idx}å±‚éªŒè¯ ---")
            
            for comp_name, comp_getter in components:
                try:
                    src_comp = comp_getter(src_block)
                    dst_comp = comp_getter(dst_layer)
                    
                    # æ£€æŸ¥ç»„ä»¶çš„æ‰€æœ‰å‚æ•°
                    src_state = src_comp.state_dict()
                    dst_state = dst_comp.state_dict()
                    
                    if set(src_state.keys()) != set(dst_state.keys()):
                        log.append(f"  âŒ {comp_name} å‚æ•°é”®ä¸åŒ¹é…")
                        layer_passed = False
                        all_passed = False
                        continue
                    
                    # æ£€æŸ¥æ¯ä¸ªå‚æ•°çš„æ•°å€¼ä¸€è‡´æ€§
                    for param_name in src_state.keys():
                        src_param = src_state[param_name]
                        dst_param = dst_state[param_name]
                        
                        consistent, max_error = parameter_consistency_check(src_param, dst_param, param_name)
                        
                        if not consistent:
                            if max_error == -1.0:
                                log.append(f"  âŒ {comp_name}.{param_name} å½¢çŠ¶ä¸åŒ¹é…")
                            else:
                                log.append(f"  âŒ {comp_name}.{param_name} æ•°å€¼ä¸ä¸€è‡´ (æœ€å¤§è¯¯å·®: {max_error:.6f})")
                            layer_passed = False
                            all_passed = False
                    # å¦‚æœæ‰€æœ‰å‚æ•°éƒ½é€šè¿‡
                    if layer_passed:
                        log.append(f"  âœ… {comp_name} éªŒè¯é€šè¿‡")
                        
                except Exception as e:
                    log.append(f"  âŒ {comp_name} éªŒè¯å¤±è´¥: {str(e)}")
                    layer_passed = False
                    all_passed = False
            
            if layer_passed:
                log.append(f"--- ç¬¬{layer_idx}å±‚æ‰€æœ‰ç»„ä»¶éªŒè¯é€šè¿‡ ---")
        
        if all_passed:
            log.append("\n===== ç¼–ç å™¨å±‚å‚æ•°è¿ç§»å…¨éƒ¨éªŒè¯é€šè¿‡ =====")
        else:
            log.append("\n===== ç¼–ç å™¨å±‚å‚æ•°è¿ç§»å­˜åœ¨é—®é¢˜ =====")
            
    except Exception as e:
        log.append(f"âŒ ç¼–ç å™¨éªŒè¯å¤±è´¥: {str(e)}")
    
    return log


def verify_decoder(rf_model: Module, target_model: Module) -> List[str]:
    """éªŒè¯ä»RFæ¨¡å‹è¿ç§»åˆ°ç›®æ ‡æ¨¡å‹decoderçš„å‚æ•°æ˜¯å¦æˆåŠŸ"""
    log = []
    log.append("\n===== Decoderå‚æ•°è¿ç§»éªŒè¯ =====")
    
    try:
        # è·å–æºå’Œç›®æ ‡decoder
        src_decoder = get_nested_module(rf_model, "transformer.decoder")
        dst_decoder = get_nested_module(target_model, "transformer.decoder")
        
        # æ£€æŸ¥æ‰€æœ‰å‚æ•°
        src_state = src_decoder.state_dict()
        dst_state = dst_decoder.state_dict()
        
        if set(src_state.keys()) != set(dst_state.keys()):
            src_keys = set(src_state.keys())
            dst_keys = set(dst_state.keys())
            log.append(f"âŒ å‚æ•°é”®ä¸åŒ¹é…")
            log.append(f"  æºæœ‰è€Œç›®æ ‡æ²¡æœ‰: {src_keys - dst_keys}")
            log.append(f"  ç›®æ ‡æœ‰è€Œæºæ²¡æœ‰: {dst_keys - src_keys}")
            return log
        
        all_passed = True
        
        # åˆ†ç»„æ£€æŸ¥å‚æ•°ï¼Œæé«˜å¯è¯»æ€§
        param_groups = {}
        for param_name in src_state.keys():
            group = param_name.split('.')[0]  # æŒ‰ç¬¬ä¸€å±‚ç»“æ„åˆ†ç»„
            if group not in param_groups:
                param_groups[group] = []
            param_groups[group].append(param_name)
        
        for group, param_names in param_groups.items():
            group_passed = True
            log.append(f"\n--- ç»„ä»¶ {group} éªŒè¯ ---")
            
            for param_name in param_names:
                src_param = src_state[param_name]
                dst_param = dst_state[param_name]
                
                consistent, max_error = parameter_consistency_check(src_param, dst_param, param_name)
                
                if not consistent:
                    if max_error == -1.0:
                        log.append(f"  âŒ {param_name} å½¢çŠ¶ä¸åŒ¹é…")
                    else:
                        log.append(f"  âŒ {param_name} æ•°å€¼ä¸ä¸€è‡´ (æœ€å¤§è¯¯å·®: {max_error:.6f})")
                    group_passed = False
                    all_passed = False
            
            if group_passed:
                log.append(f"  âœ… ç»„ä»¶ {group} æ‰€æœ‰å‚æ•°éªŒè¯é€šè¿‡")
        
        if all_passed:
            log.append("\n===== Decoderå‚æ•°è¿ç§»å…¨éƒ¨éªŒè¯é€šè¿‡ =====")
        else:
            log.append("\n===== Decoderå‚æ•°è¿ç§»å­˜åœ¨é—®é¢˜ =====")
            
    except Exception as e:
        log.append(f"âŒ DecoderéªŒè¯å¤±è´¥: {str(e)}")
    
    return log


def transfer_rfdetr_to_dinov3_weights(
    rfdetr_core_model: Module,
    dinov3_core_model: Module,
    device: torch.device = None,
    transfer_top_heads: bool = True
) -> Tuple[Module, List[str], int]:
    """
    æ ¸å¿ƒå‡½æ•°ï¼šå°†rfdetr_core_modelçš„å¯è¿ç§»æ¨¡å—æƒé‡è¿ç§»åˆ°dinov3_core_model
    å¯è¿ç§»æ¨¡å—ï¼šDecoderã€TransformerEncoderç›¸å…³æ¨¡å—ã€Backbone Projectorã€ï¼ˆå¯é€‰ï¼‰é¡¶å±‚ä»»åŠ¡å¤´
    """
    # 1. åˆå§‹åŒ–è®¾å¤‡ï¼ˆé»˜è®¤è‡ªåŠ¨é€‰æ‹©ï¼‰
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[æƒé‡è¿ç§»] ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # 2. ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹éƒ½åœ¨æŒ‡å®šè®¾å¤‡ä¸Š
    rfdetr_core = rfdetr_core_model.to(device).eval()  # evalæ¨¡å¼é¿å…BatchNormç­‰å±‚å¹²æ‰°
    dinov3_core = dinov3_core_model.to(device).eval()
    total_transferred = 0
    transfer_log: List[str] = []

    # -------------------------- 3. è¿ç§» Transformer Decoderï¼ˆå¿…è¿æ¨¡å—ï¼‰ --------------------------
    try:
        src_decoder = get_nested_module(rfdetr_core, "transformer.decoder")
        dst_decoder = get_nested_module(dinov3_core, "transformer.decoder")
        dst_decoder.load_state_dict(src_decoder.state_dict(), strict=True)
        params = count_module_params(src_decoder)
        total_transferred += params
        log = f"âœ… [Transformer Decoder] è¿ç§»å®Œæˆ | å‚æ•°é‡ï¼š{params:,}"
        transfer_log.append(log)
        print(log)
    except Exception as e:
        raise RuntimeError(f"[Transformer Decoder] è¿ç§»å¤±è´¥ï¼š{str(e)}") from e

    # -------------------------- 4. è¿ç§» Transformer Encoder ç›¸å…³æ¨¡å—ï¼ˆå¿…è¿æ¨¡å—ï¼‰ --------------------------
    encoder_related_modules: Dict[str, str] = {
        "enc_output": "transformer.enc_output",
        "enc_output_norm": "transformer.enc_output_norm",
        "enc_out_bbox_embed": "transformer.enc_out_bbox_embed",
        "enc_out_class_embed": "transformer.enc_out_class_embed"
    }
    for module_name, module_path in encoder_related_modules.items():
        try:
            src_module = get_nested_module(rfdetr_core, module_path)
            dst_module = get_nested_module(dinov3_core, module_path)
            dst_module.load_state_dict(src_module.state_dict(), strict=True)
            params = count_module_params(src_module)
            total_transferred += params
            log = f"âœ… [{module_name}] è¿ç§»å®Œæˆ | å‚æ•°é‡ï¼š{params:,}"
            transfer_log.append(log)
            print(log)
        except Exception as e:
            raise RuntimeError(f"[{module_name}] è¿ç§»å¤±è´¥ï¼š{str(e)}") from e

    # -------------------------- 5. è¿ç§» Backbone Projectorï¼ˆå¿…è¿æ¨¡å—ï¼‰ --------------------------
    try:
        # Backboneæ˜¯Joinerç±»å‹ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å®é™…Backboneå®ä¾‹ï¼ˆå«projectorï¼‰
        src_projector = get_nested_module(rfdetr_core, "backbone.0.projector")
        dst_projector = get_nested_module(dinov3_core, "backbone.0.projector")
        dst_projector.load_state_dict(src_projector.state_dict(), strict=True)
        params = count_module_params(src_projector)
        total_transferred += params
        log = f"âœ… [Backbone Projector] è¿ç§»å®Œæˆ | å‚æ•°é‡ï¼š{params:,}"
        transfer_log.append(log)
        print(log)
    except Exception as e:
        raise RuntimeError(f"[Backbone Projector] è¿ç§»å¤±è´¥ï¼š{str(e)}") from e

    # -------------------------- 6. å¯é€‰è¿ç§»ï¼šé¡¶å±‚ä»»åŠ¡å¤´ --------------------------
    if transfer_top_heads:
        top_head_modules: Dict[str, str] = {
            "class_embed": "class_embed",
            "bbox_embed": "bbox_embed",
            "refpoint_embed": "refpoint_embed",
            "query_feat": "query_feat"
        }
        for module_name, module_path in top_head_modules.items():
            try:
                src_module = get_nested_module(rfdetr_core, module_path)
                dst_module = get_nested_module(dinov3_core, module_path)
                dst_module.load_state_dict(src_module.state_dict(), strict=True)
                params = count_module_params(src_module)
                total_transferred += params
                log = f"âœ… [é¡¶å±‚ä»»åŠ¡å¤´-{module_name}] è¿ç§»å®Œæˆ | å‚æ•°é‡ï¼š{params:,}"
                transfer_log.append(log)
                print(log)
            except Exception as e:
                raise RuntimeError(f"[é¡¶å±‚ä»»åŠ¡å¤´-{module_name}] è¿ç§»å¤±è´¥ï¼š{str(e)}") from e
    else:
        log = "â„¹ï¸ [å¯é€‰é…ç½®] æœªè¿ç§»é¡¶å±‚ä»»åŠ¡å¤´ï¼ˆtransfer_top_heads=Falseï¼‰"
        transfer_log.append(log)
        print(log)

    # -------------------------- 7. è¾“å‡ºè¿ç§»æ€»ç»“ --------------------------
    summary_log = f"\n[è¿ç§»æ€»ç»“] æ€»è¿ç§»å‚æ•°é‡ï¼š{total_transferred:,} ä¸ªï¼ˆ{total_transferred / 1e6:.2f} Mï¼‰"
    transfer_log.append(summary_log)
    print("="*80)
    print(summary_log)
    print("âš ï¸  ä¸å¯è¿ç§»æ¨¡å—ï¼šbackbone.encoderï¼ˆDinoV2 vs DinoV3 ç»“æ„ä¸å…¼å®¹ï¼‰")
    print("="*80)

    return dinov3_core, transfer_log, total_transferred


def transfer_dinov3_to_core_model(
    dinov3_core_model: Module,
    pretrained_dinov3: Module,
    device: torch.device = None
) -> Tuple[Module, List[str], int]:
    """
    å°†é¢„è®­ç»ƒdinov3æ¨¡å‹çš„æƒé‡è¿ç§»åˆ°dinov3_core_modelçš„ç¼–ç å™¨ä¸­
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 1. å°†æ¨¡å‹ç§»è‡³æŒ‡å®šè®¾å¤‡
    dinov3_core_model = dinov3_core_model.to(device).eval()
    pretrained_dinov3 = pretrained_dinov3.to(device).eval()
    
    # 2. å®šä½ç›®æ ‡ç¼–ç å™¨ï¼ˆdinov3_core_modelä¸­çš„ç¼–ç å™¨ï¼‰
    try:
        dinov3_encoder = get_nested_module(dinov3_core_model, "backbone.0.encoder.encoder.encoder")
        rf_patch = get_nested_module(dinov3_core_model, "backbone.0.encoder.encoder.embeddings.patch_embeddings")
        dinov3_patch = get_nested_module(pretrained_dinov3, "patch_embed")
        
        # è¿ç§»patch embeddingå‚æ•°
        rf_patch.projection.weight.data = copy.deepcopy(dinov3_patch.proj.weight.data)
        rf_patch.projection.bias.data = copy.deepcopy(dinov3_patch.proj.bias.data)
        
        print(f"âœ… æˆåŠŸå®šä½ç¼–ç å™¨ï¼š{type(dinov3_encoder).__name__}")
    except AttributeError as e:
        raise RuntimeError(f"æ— æ³•å®šä½ç¼–ç å™¨è·¯å¾„ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„ï¼š{str(e)}") from e
    
    # 3. æå–é¢„è®­ç»ƒæ¨¡å‹çš„æ ¸å¿ƒå±‚ï¼ˆ12ä¸ªSelfAttentionBlockï¼‰
    src_blocks = pretrained_dinov3.blocks
    dst_layers = dinov3_encoder.layer  # ç›®æ ‡ç¼–ç å™¨çš„12ä¸ªå±‚
    
    # éªŒè¯å±‚æ•°é‡æ˜¯å¦åŒ¹é…ï¼ˆå‡ä¸º12å±‚ï¼‰
    assert len(src_blocks) == len(dst_layers), \
        f"æ ¸å¿ƒå±‚æ•°é‡ä¸åŒ¹é…ï¼šé¢„è®­ç»ƒæ¨¡å‹æœ‰{len(src_blocks)}å±‚ï¼Œç›®æ ‡ç¼–ç å™¨æœ‰{len(dst_layers)}å±‚"
    
    total_params = 0
    transfer_log = []
    
    # 4. é€å±‚è¿ç§»å‚æ•°ï¼ˆ12å±‚ä¸€ä¸€å¯¹åº”ï¼‰
    for i in range(len(src_blocks)):
        src_block = src_blocks[i]
        dst_layer = dst_layers[i]
        layer_log = []
        layer_total = 0
        
        # è¿ç§»å„ç»„ä»¶ï¼ˆç»“æ„å®Œå…¨åŒ¹é…ï¼‰
        components = [
            ("norm1", src_block.norm1, dst_layer.norm1),
            ("attn", src_block.attn, dst_layer.attn),
            ("ls1", src_block.ls1, dst_layer.ls1),
            ("norm2", src_block.norm2, dst_layer.norm2),
            ("mlp", src_block.mlp, dst_layer.mlp),
            ("ls2", src_block.ls2, dst_layer.ls2)
        ]
        
        for comp_name, src_comp, dst_comp in components:
            try:
                dst_comp.load_state_dict(src_comp.state_dict(), strict=True)
                params = count_module_params(src_comp)
                layer_total += params
                layer_log.append(f"{comp_name}ï¼ˆ{params:,}ï¼‰")
            except Exception as e:
                raise RuntimeError(f"ç¬¬{i}å±‚{comp_name}è¿ç§»å¤±è´¥ï¼š{str(e)}")
        
        # è®°å½•å½“å‰å±‚è¿ç§»ç»“æœ
        total_params += layer_total
        log = f"âœ… ç¬¬{i}å±‚è¿ç§»å®Œæˆ | ç»„ä»¶ï¼š{', '.join(layer_log)} | æ€»å‚æ•°é‡ï¼š{layer_total:,}"
        transfer_log.append(log)
        print(log)
    
    # 5. è¿ç§»æ€»ç»“ï¼ˆç¼–ç å™¨å·²åµŒå…¥å›dinov3_core_modelï¼‰
    summary = f"\nè¿ç§»å®Œæˆ | æ€»å‚æ•°é‡ï¼š{total_params:,}ï¼ˆ{total_params/1e6:.2f} Mï¼‰"
    transfer_log.append(summary)
    print("="*80)
    print(summary)
    print(f"ç¼–ç å™¨å·²æ›´æ–°è‡³ dinov3_core_model.backbone[0].encoder.encoder.encoder")
    print("="*80)
    
    return dinov3_core_model, transfer_log, total_params


def transfer_rf_to_dinov3(rf_model, dinov3_model):
    """
    å°†RFï¼ˆDinoV2éª¨å¹²ï¼‰æ¨¡å‹çš„å‚æ•°è¿ç§»åˆ°Dinov3æ¨¡å‹ä¸­ï¼Œä»…è¿ç§»embeddingså’Œprojectoræ¨¡å—
    """
    rf_state_dict = rf_model.state_dict()
    dinov3_state_dict = dinov3_model.state_dict()
    new_dinov3_state = OrderedDict()

    # å‚æ•°æ˜ å°„ï¼šä»…åŒ…å«embeddingså’Œprojectoræ¨¡å—ï¼Œç§»é™¤æ‰€æœ‰encoder.layerç›¸å…³æ˜ å°„
    param_mapping = {
        # -------------------------- embeddingsæ¨¡å—ï¼ˆæ ¸å¿ƒè¿ç§»ï¼‰--------------------------
        # ä½ç½®ç¼–ç 
        "encoder.encoder.embeddings.position_embeddings": 
            "encoder.encoder.embeddings.position_embeddings",
        # CLSä»¤ç‰Œ
        "encoder.encoder.embeddings.cls_token": 
            "encoder.encoder.embeddings.cls_token",
        # æ©ç ä»¤ç‰Œ
        "encoder.encoder.embeddings.mask_token": 
            "encoder.encoder.embeddings.mask_token",
        # å¯„å­˜å™¨ä»¤ç‰Œ
        "encoder.encoder.embeddings.register_tokens": 
            "encoder.encoder.embeddings.register_tokens",
        # è¡¥ä¸åµŒå…¥
        "encoder.encoder.embeddings.patch_embeddings.projection.weight": 
            "encoder.encoder.embeddings.patch_embeddings.projection.weight",
        "encoder.encoder.embeddings.patch_embeddings.projection.bias": 
            "encoder.encoder.embeddings.patch_embeddings.projection.bias",
        
        # -------------------------- æœ€ç»ˆå±‚å½’ä¸€åŒ–ï¼ˆélayeréƒ¨åˆ†ï¼‰--------------------------
        "encoder.encoder.layernorm.weight": 
            "encoder.encoder.layernorm.weight",
        "encoder.encoder.layernorm.bias": 
            "encoder.encoder.layernorm.bias",
        
        # -------------------------- Projectoræ¨¡å— --------------------------
        "projector.stages.0.0.cv1.conv.weight": 
            "projector.stages.0.0.cv1.conv.weight",
        "projector.stages.0.0.cv1.bn.weight": 
            "projector.stages.0.0.cv1.bn.weight",
        "projector.stages.0.0.cv1.bn.bias": 
            "projector.stages.0.0.cv1.bn.bias",
        "projector.stages.0.0.cv2.conv.weight": 
            "projector.stages.0.0.cv2.conv.weight",
        "projector.stages.0.0.cv2.bn.weight": 
            "projector.stages.0.0.cv2.bn.weight",
        "projector.stages.0.0.cv2.bn.bias": 
            "projector.stages.0.0.cv2.bn.bias",
        # Bottleneckæ¨¡å—ï¼ˆ3ä¸ªï¼‰
        "projector.stages.0.0.m.{m}.cv1.conv.weight": 
            "projector.stages.0.0.m.{m}.cv1.conv.weight",
        "projector.stages.0.0.m.{m}.cv1.bn.weight": 
            "projector.stages.0.0.m.{m}.cv1.bn.weight",
        "projector.stages.0.0.m.{m}.cv1.bn.bias": 
            "projector.stages.0.0.m.{m}.cv1.bn.bias",
        "projector.stages.0.0.m.{m}.cv2.conv.weight": 
            "projector.stages.0.0.m.{m}.cv2.conv.weight",
        "projector.stages.0.0.m.{m}.cv2.bn.weight": 
            "projector.stages.0.0.m.{m}.cv2.bn.weight",
        "projector.stages.0.0.m.{m}.cv2.bn.bias": 
            "projector.stages.0.0.m.{m}.cv2.bn.bias",
        # Projectoræœ€ç»ˆå±‚å½’ä¸€åŒ–
        "projector.stages.0.1.weight": 
            "projector.stages.0.1.weight",
        "projector.stages.0.1.bias": 
            "projector.stages.0.1.bias",
    }

    # 1. å¤„ç†Projectorä¸­çš„Bottleneckæ¨¡å—ï¼ˆå«{m}å¾ªç¯å˜é‡ï¼‰
    num_bottlenecks = 3  # å›ºå®š3ä¸ªBottleneck
    for m in range(num_bottlenecks):
        for dinov3_key_pattern, rf_key_pattern in param_mapping.items():
            if "{m}" not in dinov3_key_pattern:
                continue  # åªå¤„ç†å«{m}çš„å‚æ•°
            
            dinov3_key = dinov3_key_pattern.format(m=m)
            if dinov3_key not in dinov3_state_dict:
                continue  # Dinov3ä¸­æ— æ­¤å‚æ•°
            
            rf_key = rf_key_pattern.format(m=m)
            if rf_key not in rf_state_dict:
                continue  # RFä¸­æ— æ­¤å‚æ•°
            
            # éªŒè¯å½¢çŠ¶å¹¶è¿ç§»
            if rf_state_dict[rf_key].shape == dinov3_state_dict[dinov3_key].shape:
                new_dinov3_state[dinov3_key] = rf_state_dict[rf_key]
                print(f"âœ… è¿ç§»Bottleneck {m}ï¼š{dinov3_key}")
            else:
                print(f"âš ï¸ è·³è¿‡Bottleneck {m}ï¼š{dinov3_key}ï¼ˆå½¢çŠ¶ä¸åŒ¹é…ï¼‰")

    # 2. å¤„ç†æ— å¾ªç¯å˜é‡çš„æ¨¡å—ï¼ˆembeddingså’Œå›ºå®šç»“æ„ï¼‰
    for dinov3_key, rf_key in param_mapping.items():
        # è·³è¿‡å«å¾ªç¯å˜é‡çš„å‚æ•°ï¼ˆå·²å¤„ç†ï¼‰
        if "{m}" in dinov3_key:
            continue
        
        if dinov3_key not in dinov3_state_dict:
            continue  # Dinov3ä¸­æ— æ­¤å‚æ•°
        
        # ä»RFè·å–å‚æ•°ï¼ˆéå‡½æ•°ç±»å‹æ˜ å°„ï¼‰
        if not callable(rf_key) and rf_key in rf_state_dict:
            rf_param = rf_state_dict[rf_key]
            # éªŒè¯å½¢çŠ¶
            if rf_param.shape == dinov3_state_dict[dinov3_key].shape:
                new_dinov3_state[dinov3_key] = rf_param
                print(f"âœ… è¿ç§»ï¼š{dinov3_key}")
            else:
                print(f"âš ï¸ è·³è¿‡ï¼š{dinov3_key}ï¼ˆå½¢çŠ¶ä¸åŒ¹é…ï¼ŒRF: {rf_param.shape}, Dinov3: {dinov3_state_dict[dinov3_key].shape}ï¼‰")


    # 3. ä¿ç•™Dinov3æ‰€æœ‰æœªåŒ¹é…çš„å‚æ•°ï¼ˆå°¤å…¶æ˜¯encoder.layerç›¸å…³å‚æ•°ï¼‰
    for dinov3_key in dinov3_state_dict:
        if dinov3_key not in new_dinov3_state:
            new_dinov3_state[dinov3_key] = dinov3_state_dict[dinov3_key]

    # åŠ è½½è¿ç§»åçš„å‚æ•°
    dinov3_model.load_state_dict(new_dinov3_state, strict=False)
    print("\nğŸ‰ è¿ç§»å®Œæˆï¼å·²è·³è¿‡æ‰€æœ‰encoder.layerç›¸å…³å‚æ•°")
    return dinov3_model

def initialize_dinov3_weights(model):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            nn.init.constant_(module.bias, 0)
            nn.init.constant_(module.weight, 1)
        elif hasattr(module, 'reset_parameters'):
            module.reset_parameters()

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    plus = ""
    save_path = f'medium-dinov3{plus}-randomdecoder.pth'
    
    # åŠ è½½æ¨¡å‹
    if plus == "plus":
        model = RFDETRMediumV3Plus()
    else:
        model = RFDETRMediumV3()

    # rfdetr = RFDETRMedium(pretrain_weights='D:/__easyHelper__/RF-DETR/rfdetr/checkpoint/medium-coco.pth')
    dinov3 = torch.hub.load(
        'D:/__easyHelper__/dinov3-main', 
        f'dinov3_vits16{plus}', 
        source='local', 
        weights=f'D:/__easyHelper__/dinov3-main/checkpoint/dinov3_vits16{plus}.pth'
    )
    
    # rfdetr_core_model = rfdetr.model.model.to(device)
    dinov3_core_model = model.model.model.to(device)
    
    # æ‰§è¡Œè¿ç§»æ­¥éª¤
    # print("===== å¼€å§‹è¿ç§»RF-DETRåˆ°DinoV3æ ¸å¿ƒæ¨¡å‹ï¼ˆéencoderéƒ¨åˆ†ï¼‰ =====")
    # dinov3_core_model, transfer_log, total_params = transfer_rfdetr_to_dinov3_weights(
    #     rfdetr_core_model=rfdetr_core_model,
    #     dinov3_core_model=dinov3_core_model,
    #     device=None,
    #     transfer_top_heads=False
    # )
    
    initialize_dinov3_weights(dinov3_core_model)

    print("\n===== å¼€å§‹è¿ç§»é¢„è®­ç»ƒDinoV3åˆ°æ ¸å¿ƒæ¨¡å‹ç¼–ç å™¨ =====")
    updated_core_model, transfer_log, total_params = transfer_dinov3_to_core_model(
        dinov3_core_model=dinov3_core_model,
        pretrained_dinov3=dinov3,
        device=None
    )
    
    # ä¿å­˜æ¨¡å‹å‰è¿›è¡ŒéªŒè¯
    print("\n===== å¼€å§‹è¿ç§»åéªŒè¯ =====")
    
    # éªŒè¯Encoderå‚æ•°ï¼ˆæ¥è‡ªé¢„è®­ç»ƒDinoV3ï¼‰
    encoder_logs = verify_encoder_layers(dinov3, updated_core_model)
    for log in encoder_logs:
        print(log)
    
    
    # ä¿å­˜éªŒè¯é€šè¿‡çš„æ¨¡å‹
    torch.save({
        'model': dinov3_core_model.state_dict(),
    }, save_path)
    print(f"\næ¨¡å‹å·²ä¿å­˜è‡³ {save_path}")
