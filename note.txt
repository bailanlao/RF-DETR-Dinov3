# 训练时使用的是model.model.train
# train函数保存的实际上是model.model.model，最后mae预训练完成保存model.model.model

WindowedDinov2WithRegistersEncoder(
  (layer): ModuleList(
    (0-11): 12 x WindowedDinov2WithRegistersLayer(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attention): Dinov2WithRegistersSdpaAttention(
        (attention): Dinov2WithRegistersSdpaSelfAttention(
          (query): Linear(in_features=384, out_features=384, bias=True)
          (key): Linear(in_features=384, out_features=384, bias=True)
          (value): Linear(in_features=384, out_features=384, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (output): Dinov2WithRegistersSelfOutput(
          (dense): Linear(in_features=384, out_features=384, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (layer_scale1): Dinov2WithRegistersLayerScale()
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Dinov2WithRegistersMLP(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (activation): GELUActivation()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
      )
      (layer_scale2): Dinov2WithRegistersLayerScale()
    )
  )
)
dinov3
DinoVisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (rope_embed): RopePositionEmbedding()
  (blocks): ModuleList(
    (0-11): 12 x SelfAttentionBlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): SelfAttention(
        (qkv): LinearKMaskedBias(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): LayerScale()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (ls2): LayerScale()
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (head): Identity()
)